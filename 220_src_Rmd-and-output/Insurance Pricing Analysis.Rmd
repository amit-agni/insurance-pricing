---
title: "Insurance Pricing Analysis"
author: "Amit Agni"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning = F,error = F)

options(scipen=9) #remove scientific notation in printing 

if(!require(pacman)) { install.packages("pacman"); library(pacman)} #for p_load()
pacman::p_load(data.table #data store and wrangling
       ,here # alternative to setwd(), helps in project portabilty as it automatically constructs the file path based on the R project directory structure
       ,magrittr # for the %>% piping operator
       ,dplyr # for na_if() function
       ,tictoc #display start-end times
       ,DataExplorer #plotting functions
       ,scales #for scale and center function
       ,tidyr #data manipulation
       ,purrr #Mapping functions
       ,mlr # Machine learning library
       ,ggplot2 #plotting
       ,kableExtra #table formating
       ,formattable #table formating
       ,caret #for dummy vars
       ,FSelectorRcpp #variable importance 
       )

source(here::here("210_src_R-scripts-functions","functions","misc_functions.R")) #chr2fact helper function

#snippet for tableformatting
fn_format_tbl <- function(x){
  kable(x,digits = 2) %>% kable_styling(bootstrap_options = 'condensed'
                ,full_width = F
                ,position = 'left'
                ) %>%
    row_spec(1,bold = F)
}


```

***

### Objective

The objective of the task is to analyse the pricing of car insurance premiums of Suncorp against its three competitors. The dataset consists of factors that affect the pricing of insurance premiums like vehicle details, owner / driver information, the details of other insurances on the vehicle, etc.

We will do some basic cleaning of the data and then carry out exploratory data analysis. To explore the relationship between the Insurance prices and the feature set we will create a minimal Shiny App and host it [here](https://amitagni.shinyapps.io/Premium_Pricing_Factors/) . We will also build a simple pricing model and provide recommendations to SunCorp

***



### Data Cleaning

As a first step, we will carry out some basic cleaning 

#### Eliminate Duplication

* The Policy Commencement field has only 2 dates and both have same count of records.
* The sum,mean and standard deviation of PREMIUM columns also gives same values for all the insurers except for INSURER2 
  + This could possibly mean that the three insurers (1,3 and 4) have not done a pricing review between the 2 dates
  + The average price increase for INSURER2 is around 2% 
* To avoid duplication we will only consider the 1st Sep 2019 records for our analysis


```{r}
#Load data
DT <- fread(here('100_data_raw-input','data.csv'))

#Character to Date format
DT[,COMMENCEDATE := lubridate::ymd(COMMENCEDATE)]

#both dates have duplicate rows, except for 'INSURER2_PREMIUM'
col_names <- grep('PREMIUM',names(DT),value = T)

rbindlist(list(Count = DT[,lapply(.SD, length),by=COMMENCEDATE,.SDcols = col_names] 
               ,Sum = DT[,lapply(.SD, sum),by=COMMENCEDATE,.SDcols = col_names] 
          ,Mean = DT[,lapply(.SD, mean),by=COMMENCEDATE,.SDcols = col_names]
          ,`Std. Dev` = DT[,lapply(.SD, sd),by=COMMENCEDATE,.SDcols = col_names]
          
          ),fill = T,idcol = 'Type')   %>% fn_format_tbl() %>%
    column_spec(4, bold = F, color = "tomato") 

#Keep only 1st Sep 19 records
DT <- DT[COMMENCEDATE == '2019-09-01']
DT$COMMENCEDATE <- NULL

premium_cols <- grep('PREMIUM',names(DT),value = T)

```

#### Columns with redundant information

* These column pairs do not provide additional information, we will drop one of them from the dataset
  + The two columns 'At fault in 5 years'  and 'At fault in 2 years' have same data distribution
  + Similarly the VEH AGE is derived using MANUFYEAR (with 2019 as base)
  + The MVINSYEARS column also has a high correlation with NRMASST colum


```{r results='hide'}
#Both cols give same info, drop one
table(DT$ATFAULTACCDS,DT$ATFAULT5YRS) %>% fn_format_tbl()
DT$ATFAULTACCDS <- NULL

table(DT$VEHAGE,DT$MANUFYEAR)  %>% fn_format_tbl()
DT$MANUFYEAR <- NULL

table(DT$MVINSYEARS)
table(DT$NRMAASST)

DT$MVINSYEARS <- NULL

```

#### Combine rare levels
* The distribution of Car MAKE is top heavy, ie the top 13 car companies contribute towards 90% of the total records (volume and not value), so we will retain only these 13 MAKEs and merge the rest as 'OTHERS'
* And then keep top 3 models under each of the MAKEs
* This is done as majority of the ML models do not handle high number of data levels. Also, by merging these rare levels we do not 'expect' to lose much information
* BEFORE : MAKE count: `r length(unique(DT$MAKE))` and MODEL count :  `r length(unique(DT$MODEL))`
```{r}

#combine rare makes
temp <- DT[,.N,.(MAKE)][order(-N)][,.(MAKE,N,cumN = cumsum(N))][cumN < 0.90 * nrow(DT)]
DT <- merge(DT,temp[,-"cumN"],by=c("MAKE"),all.x = T)
DT[,`:=`(MAKE = if_else(is.na(N),'OTHERS',MAKE)
         ,N = NULL)]

#combine rare models, keep top 3
temp <- DT[,.N,.(MAKE,MODEL)][order(MAKE,-N)][,head(.SD, 3),.(MAKE)]
DT <- merge(DT,temp,by=c("MAKE","MODEL"),all.x = T)
DT[,`:=`(MODEL = if_else(is.na(N),'OTHERS',MODEL)
         ,N = NULL)]

rm(temp)

```
* AFTER : MAKE count: `r length(unique(DT$MAKE))` and MODEL count :  `r length(unique(DT$MODEL))`

#### Other Data Manipulations
* We will convert the character columns to factors
* And also few other manipulations for readbility and to maintain consistency

```{r}
#for readability/consistency
#table(DT$NRMAASST)
DT[NRMAASST <= 0, NRMAASST := 0]
DT[,LICCANCNT := as.factor(if_else(LICCANCNT == 0,'N','Y'))]

#Some int cols can be changed to factors
col_names <- c('NCBPCT','NRMAASST','YEARLYKM')
DT[,(col_names) := lapply(.SD,as.factor),.SDcols = col_names]


#char cols to factor
DT <- chr2fact(DT)

DT[,DEMERITPTS := as.factor(case_when(DEMERITPTS == 0 ~ 'No'
                                          ,DEMERITPTS>0 & DEMERITPTS<=5~ 'Low'
                                          ,DEMERITPTS>5 ~ 'High'
                                          ,TRUE ~ 'Err'))]
DT[,NOYRLICOBT := as.factor(case_when(NOYRLICOBT < 1 ~ 'Low'
                                          ,NOYRLICOBT>=1 & NOYRLICOBT<=5~ 'Med'
                                          ,NOYRLICOBT>5 ~ 'High'
                                          ,TRUE ~ 'Err'))]

```

***

### Exploratory Analysis

Now lets look at the individual feautures

#### Numeric Features

* The three numeric 'age' features have an almost normal distribution
  + This is expected as majority of the people drive in their mid-ages. 
  + The record count of vehicle age is nearly constant from 4 to 12 years, after which it starts dropping. This probably means that new cars are insured by a different providers or it could also mean that people have switched their insurance companies for their existing older cars. We would get a better idea if the volumes were provided.
  + For modeling purposes, these columns could be converted to bins but for now we will leave them as is
  

```{r fig.width=12, fig.align='left'}
plot_histogram(DT[,-..premium_cols])

```

#### Categorical Features
* The below charts gives the distrbution of the categorical features
  + Due to the rare-level merging that we did earlier the 'OTHERS' category in the MODEL feature now has high frequency of records
  + The LICCANCNT has a very low proportion (`r round(prop.table(table(DT$LICCANCNT))[2],4)`) of records and could ideally be ignored. But since it contains the conviction/suspension information we will retain it.
  


```{r fig.width=12,fig.height=16, fig.align='left'}
plot_bar(DT,maxcat = 100,nrow = 10,order_bar = T,ncol = 4)

```

#### Feature Correlations

+ We do not see much collinearity in the feature set as we have already done some cleaning in the previous steps
+ We can see that the Owner Age and Youngest driver age is highly correlated, possibly an indication that the majority of the car owners are the sole drivers of the vehicle
+ The MVINSTYPE = N has a high correlaion with MVINSURER= NONE, but we will not treat it
+ Assuming the Insurance Premiums to be the target variables, we notice they have slight correlations with the feature set. We will explore this further in the feature importance section

```{r fig.width=20, fig.height=20,fig.align='left'}

data <- DT[,-c('MAKE','MODEL')]
dummy = dummyVars(~ ., data = data, fullRank = T)
data = data.frame(predict(dummy, newdata = data))

correl <- cor(data)
ggcorrplot::ggcorrplot(correl, hc.order = FALSE, type = "lower",lab=TRUE,lab_size = 4,digits = 1
                      ,show.legend = F
                      ,ggtheme = 'theme_classic')

```

***

### Insurance Premium columns 

In this section, we will take a deeper look at how the four insurers are pricing their products?

#### Frequency Distribution

* The premium prices for all the insurers are in the range of 190-390
```{r}
summary(DT[,..premium_cols]) %>% fn_format_tbl()

```

* Below box plots shows their distributions. We can see that:
  * The premium prices of Insurer 2 and 3 are higher on average
    + Insurer 2 has a wider IQR, which is a good strategy as their prices would provide more/customised options to their customers. This could possibly result in higher revenue
    + Insurer 3 has the highest median price
  * Insurer 1 and 4 are similar in their pricing 
    + They both have a lower median price of around 210-220, which is much lower than their competitors.
    + And they also have outliers
  
```{r fig.width=12}
DT[,..premium_cols] %>%
  melt() %>%
  ggplot(aes(x=variable,y=value)) +
  geom_boxplot() 

```

* Looking at histograms 
  + It looks like the prices of Insurer 2 and 3 are capped to an upper limit, probably as a result of some outlier treatment
  + The frequency distribution of Insurer 1 and 4 is not normal which indicates they have a pricing strategy which is based on price slabs. (ie the pricing is done in steps) 
  + As compared to the pricing of Insurer 2 and 3, which is more dynamic and smooth
  
```{r fig.width=12}
plot_histogram(DT[,..premium_cols])

```

#### Feature Importance

* Now, we will look at the factors that each INSURER uses to arrive at their premium pricing
  + We could have possibly used tree algorithms like RandomForest or Xgboost as they have give feature importance
  + But we will use the entropy and information gain algorithm from the FSelectorRcpp package to determine the feature importance. 
[Source](http://mi2-warsaw.github.io/FSelectorRcpp/articles/get_started.html)

* We can see that INSURER 1 and 4 give high weightage to the age features to arrive at their pricing, compared to the others
* Alongwith the age column, the pricing of Insurer 2 also considers features like Demerit points and average annual kilometer

*Table gives the top 10 features for each Insurer (MAKE and MODEL columns were ignored)*
```{r fig.width=12, fig.align='left'}

v_imps <- list()
for(col in premium_cols){
  other_ins_cols <- premium_cols[!premium_cols %in% col]

  data <- DT[,-c('MAKE','MODEL',..other_ins_cols)]
  dummy = dummyVars(~ ., data = data, fullRank = T)
  data = data.frame(predict(dummy, newdata = data))

  train_task <- makeRegrTask(data=data,target = col)
  
  fv = generateFilterValuesData(train_task, method = c('FSelectorRcpp_information.gain')
                                ,more.args = list(FSelectorRcpp_information.gain = c(equal = T)))

  v_imps[[col]] <- list(fv = fv)

}

v_imps_table <- map(v_imps,`[[`,'fv') %>% map(`[[`,'data') %>%  rbindlist(idcol = 'ins')

v_imps_table[order(ins,method,-value)][,head(.SD,10),.(ins,method)][
  method == 'FSelectorRcpp_information.gain'] %>%
  dcast(name~ins,value.var = 'value') %>% .[order(-INSURER1_PREMIUM)] %>% fn_format_tbl()

```


* Below are the variable importance plots for all the four insurers (features not sorted)


```{r fig.width=12}
#plots
plots <- map(v_imps,`[[`,'fv') %>% map(.
                              ,function(x){ 
                                plotFilterValues(x,sort = "none") +
                                  ggtitle(map(x,`[[`,'target'))
                                })

for(i in 1:4) print(plots[[i]])

```

#### Pricing of YDAGE column

From the above analysis we can see that YDAGE is an important feature. Lets look into more details

* Below boxplots shows how the different insurers price their products for different 'Youngest Driver' age groups
  + If we compare the distribution of Insurer 1 and 4 with that of 2 and 3, we can see that Insurer 2 and 3 have much smoother price curves
  + Insurer 1 and 4 charge uniformly for different age groups
* Similar plots for other top features (not shown) also indicates that the pricing strategy of Insurer1 is not dynamic
  + **All the remaining features can be explored using the [Shiny App](https://amitagni.shinyapps.io/Premium_Pricing_Factors/) hosted on shiny.io**



```{r fig.width=12}

DT[,c('YDAGE',..premium_cols)] %>% 
      melt(id.vars = 'YDAGE')   %>%
      ggplot(aes(y=value,x = YDAGE,group = YDAGE)) +
      geom_boxplot() +
      facet_wrap(~variable)

```

***

### Pricing Model

We will build a very basic pricing model for Insurer 1 using XGBoost algorithm

#### Process
  + We will use the prices of Insurer1's competitors as Target variable
  + The model learns from the pricing strategies applied by the competitors and comes out with the prices that can be used by Insurer1
  + Insurer1 could potentially use the predicted prices to conduct an A/B testing and measure the impact on its sales and revenue


#### Model Evaluation
* If we look at the below boxplots, we can see that the model predicts a median price which is higher than its existing price and lower than Insurer 2 and 3
* The distribution of the prices is approximately normal as seen in the histogram


#### Limitations

* Prior Sales and the Market Demand are the key factors used in the pricing of any products. The pricing models for vehicle insurance products also heavily rely on past accidents and claims, none of which were available for modeling
* Key model improvement strategies like feature engineering, resampling, hyperparameter tuning, model ensembling, etc were not utilised
* **The pricing structure predicted by the model is better than the one currently used by Insurer1, but the model itself is nothing better than an average model and only included here as a POC (Proof of Concept)**


```{r eval = F}

#Melt premiums of all the competitors 
DT_train <- DT %>% melt(measure.vars = premium_cols)

#Held-out Insurer 1 records 
DT_predict <- DT_train[variable == 'INSURER1_PREMIUM']
DT_train <- DT_train[variable != 'INSURER1_PREMIUM']

DT_train <- droplevels(DT_train)

#remove the insurance provider identifier
DT_train$variable <- NULL

#Dedup
DT_train <- DT_train[!duplicated(DT_train)]

p_load(parallelMap)
parallelStartSocket(4) #for parallel operation


# Modeling parameters
k <- 5L #K-fold Cross validation
random_grid_iters <-50L #Hyperparameter tuning grid rows

#Modeling using MLR package
train_task <- makeRegrTask(data=DT_train
                              ,target = "value") %>% 
  normalizeFeatures(cols = c('OWNERAGE','YDAGE','VEHAGE')) %>%
  createDummyFeatures(method = 'reference')

### XGB Model
learner <- makeLearner("regr.xgboost"
                       ,predict.type = "response"
                       )

param_set <- makeParamSet(makeIntegerParam("nrounds",lower=100,upper=500)
                          ,makeIntegerParam("max_depth",lower=3,upper=8)
                          #,makeNumericParam("lambda",lower=0.55,upper=0.60)
                          ,makeNumericParam("eta", lower = 0.01, upper = 0.3)
                          ,makeNumericParam("subsample", lower = 0.1, upper = 0.80)
                          #,makeNumericParam("min_child_weight",lower=1,upper=10)
                          ,makeNumericParam("colsample_bytree",lower = 0.1,upper = 0.8))

param_rand <- makeTuneControlRandom(maxit = random_grid_iters) #Grid

cv <- makeResampleDesc("CV"
                       ,iters = k #k fold CV
                       ,predict = "both") # generate performance on the train data along with the validation/test data

model_tune <- tuneParams(learner = learner
                         ,resampling = cv
                         ,task = train_task
                         ,par.set = param_set
                         ,control = param_rand
                         ,measures = list(rmse, setAggregation(rmse,train.mean)) #aggregate train and test rmse
)


best_param <- setHyperPars(learner,par.vals = model_tune$x)
model <- mlr::train(best_param,train_task)

# create test task for the predict function
DT_predict <- droplevels(DT_predict)
DT_predict$variable <- NULL
test_task <- makeRegrTask(data=DT_predict
                              ,target = "value") %>% 
  normalizeFeatures(cols = c('OWNERAGE','YDAGE','VEHAGE')) %>%
  createDummyFeatures(method = 'reference')

pred <- predict(model,test_task)

saveRDS(pred,'predicted_values.RDS')

```


```{r fig.width=12}

#Read from the pre-saved predictions
pred <- readRDS(here('110_data_intermediate','predicted_values.RDS'))

DT$INSURER1_PREMIUM_NEW <- pred$data$response

premium_cols <- grep('PREMIUM',names(DT),value = T)

#DT[,c('INSURER1_PREMIUM','INSURER1_PREMIUM_NEW')] %>%
DT[,..premium_cols] %>%
  melt() %>%
  ggplot(aes(x=variable,y=value)) +
  geom_boxplot() 

plot_histogram(DT[,c('INSURER1_PREMIUM','INSURER1_PREMIUM_NEW')])


```

***

### Summary

* Insurer1 is currently having a pricing strategy in which their pricing is done by slabs. This means the customers will have less buying options to choose from. If they adopt a dynamic pricing (with a smoother pricing curve), it could help in generating incremental revenue
* The pricing of Insurer1 is majorly based on the Age columns, they could consider other available factors 
* The insurer can probably conduct A/B testing using the prices predicted by basic model presented above and measure its impact on sales and revenue.


***
